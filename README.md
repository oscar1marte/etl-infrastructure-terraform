# ETL Infrastructure Challenge

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

This project is part of a technical challenge designed to evaluate proficiency in designing, implementing, and managing ETL pipelines using AWS and Snowflake, with Terraform to manage infrastructure as code.

## Project Structure
```sh
├── 0_terraform_setup
│   ├── backend.tf
│   ├── main.tf
│   ├── outputs.tf
│   ├── providers.tf
│   ├── variables.tf
│   ├── variables.tfvars
│   ├── modules/
│   │   ├── credential/
│   │   │   ├── main.tf
│   │   │   ├── variables.tf
│   │   ├── s3/
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── variables.tf
│   │   ├── snowflake/
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── variables.tf
│   └── data/
│       └── customer_segmentation_data.csv
├── terraform_setup/
│   ├── backend.tf
│   ├── main.tf
│   ├── outputs.tf
│   ├── providers.tf
│   ├── variables.tf
│   ├── variables.tfvars
│   ├── modules/
│   │   ├── glue/
│   │   │   ├── main.tf
│   │   │   ├── variables.tf
│   │   ├── iam/
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── variables.tf
│   │   ├── s3/
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── variables.tf
│   │   ├── secrets_manager/
│   │   │   ├── main.tf
│   │   │   ├── outputs.tf
│   │   │   ├── variables.tf
│   └── data/
│       ├── etl_job.py
│       ├── snowflake_connector_python-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
```
This project includes two separate Terraform setups:
- **0_terraform_setup:**
The purpose of the **0_terraform_setup** directory is to create the initial setup required for this challenge. Rather than manually setting up the foundational resources (such as Snowflake configurations and basic S3 bucket structure), Terraform is used to automate this process. This setup ensures that all required prerequisites for the challenge are in place before the main infrastructure is deployed.
- **terraform_setup:**
The terraform_setup directory contains the main challenge setup. This is where the primary infrastructure for the ETL pipeline is defined and deployed. It includes the creation of AWS resources (such as S3 buckets, IAM roles, Glue jobs, and Secrets Manager secrets) and the necessary Snowflake resources. This setup automates the deployment of the infrastructure needed to run the ETL pipeline, manage data, and load it into Snowflake.


## Prerequisites
Before you begin, ensure you have the following:
- **Terraform:** Installed on your local machine.
- **AWS Account:** With necessary permissions to create and manage S3, IAM, Glue, and Secrets Manager resources.
- **Snowflake Account:** With admin rights to create databases, schemas, and users.

## Setup Instructions
### Step 1: Configure **0_terraform_setup** Variables
Fill in the necessary variables in the **variables.tfvars** file:
```sh
environment             = "dev"                                 
snowflake_account       = "your_snowflake_account"              # Required - Example: bcv48376 (Run SELECT CURRENT_ACCOUNT(); on Snowflake)
snowflake_username      = "your_snowflake_username"             # Required
snowflake_password      = "your_snowflake_password"             # Required
snowflake_role          = "accountadmin"                    
snowflake_host          = "your_snowflake_host"                 # Required - Example: ereyenh-ayt44551.snowflakecomputing.com
snowflake_warehouse     = "COMPUTE_WH"
snowflake_database_name = "CHALLENGE_DATABASE"
snowflake_schema_name   = "CUSTOMER_SEGMENTATION"
snowflake_user_role_name= "AWSETL"
snowflake_user_name     = "AWSUSER"
snowflake_stage_name    = "FILES_STAGE"
aws_region              = "us-east-1"
aws_access_key          = "your_aws_access_key"                 # Required
aws_secret_key          = "your_aws_secret_key"                 # Required
aws_s3_bucket_name      = "sandex-data-challenge-bucket"                 
aws_s3_bucket_tag_name  = "Sandex Data Challenge Bucket"             
csv_file_name           = "customer_segmentation_data.csv"
```

### Step 2: Deploy **0_terraform_setup** Infrastructure
##### 1. Initialize Terraform:
#
```sh
cd 0_terraform_setup
terraform init
```
##### 2. Run plan Terraform to validate:
#
```sh
terraform plan -var-file="variables.tfvars"
```
##### 3. Apply the Terraform Configuration:
#
```sh
terraform apply -var-file="variables.tfvars"
```
When you apply this Terraform configuration, it will automatically generate a JSON file named **snowflake_credential.json**. This file contains the Snowflake **username** and a **password** (needed for the next **terraform_setup**) that is dynamically generated by Terraform. 

### Step 3: Configure **terraform_setup** Variables
Fill in the necessary variables in the **variables.tfvars** file:
```sh
environment               = "dev"
aws_s3_bucket_name        = "sandex-data-challenge-bucket-dev"
csv_object_name           = "customer_segmentation_data.csv"
aws_region                = "us-east-1"
aws_access_key            = "your_aws_access_key"                                     # Required
aws_secret_key            = "your_aws_secret_key"                                     # Required
aws_etl_script            = "etl_job.py"
aws_glue_role_name        = "AWSGlueCustomRole"
etl_job_name              = "etl-challenge-glue-job"
snowflake_secrets_manager = "snowflake_secrets"                                       # If you create a secrets_manager, and destroy it, later to create again another secrets_manager you will need to change a bit this name. (I think AWS doesn't remove completly the resource)
snowflake_username        = "AWSUSER"
snowflake_password        = "your_snowflake_password"                                 # Required (Generated in the previous terraform)
snowflake_account         = "your_snowflake_account"                                  # Required - Example: ereyenh-ayt44551 (It's different from the snowflake_account from previous terraform, in this case, it's the first part of the previous host, it's a little weird)
snowflake_warehouse       = "COMPUTE_WH"
snowflake_database        = "CHALLENGE_DATABASE"
snowflake_schema          = "CUSTOMER_SEGMENTATION"
snowflake_role            = "AWSETL"
snowflake_stage           = "@FILES_STAGE"
```
### Step 4: Deploy **terraform_setup** Infrastructure
##### 1. Initialize Terraform:
#
```sh
cd terraform_setup
terraform init
```
##### 2. Run plan Terraform to validate:
#
```sh
terraform plan -var-file="variables.tfvars"
```
##### 3. Apply the Terraform Configuration:
#
```sh
terraform apply -var-file="variables.tfvars"
```
By using Terraform for both setups, the entire process is automated, ensuring consistency and minimizing the risk of manual errors. The first setup (0_terraform_setup) prepares the groundwork, including generating essential credentials, while the second setup (terraform_setup) builds the full infrastructure necessary for the challenge.

### Step 5: Run the ETL Job
**Trigger the ETL Job:**
The ETL job will automatically extract, transform, and load data into Snowflake once the infrastructure is set up.
Monitor the job's progress through AWS Glue and CloudWatch logs.
**Note:** We could create a schedule using **CloudWatch**, or an event for **S3** along with a **lambda** to run the ETL job whenever there are changes.

## Challenge (Ensuring Data Integrity During ETL)
**Issue:** Ensuring that data extracted from S3 and transformed using Python was accurately loaded into Snowflake while maintaining data integrity.
**Solution:** To validate the integrity of the data throughout the ETL process, a hashing mechanism was implemented. This involves generating hash values for the data before and after the process, and then comparing these hashes to ensure that the data remains consistent and unaltered during the ETL process. Additionally, SQL queries were executed post-load in Snowflake to verify that the data was accurately loaded into the tables.

### SQL Accurately queries
The following SQL queries are designed to be executed in Snowflake to accurately validate the accuracy and integrity of the data loaded into the Snowflake tables:
**Note:** Only some queries are shown. Since the queries are very similar, only the column names and table references change.
```sh
# Validate Foreign Key Relationship (Example using customer_behavior and customer_demographics tables. The query should return zero rows)
SELECT cb.CUSTOMER_ID
FROM CHALLENGE_DATABASE.CUSTOMER_SEGMENTATION_DEV.CUSTOMER_BEHAVIOR cb
LEFT JOIN CHALLENGE_DATABASE.CUSTOMER_SEGMENTATION_DEV.CUSTOMER_DEMOGRAPHICS cd
ON cb.CUSTOMER_ID = cd.CUSTOMER_ID
WHERE cd.CUSTOMER_ID IS NULL;

# Check for Null Values in Mandatory Columns (Example using customer_behavior table)
SELECT *
FROM CHALLENGE_DATABASE.CUSTOMER_SEGMENTATION_DEV.CUSTOMER_BEHAVIOR
WHERE CUSTOMER_ID IS NULL
   OR BEHAVIORAL_DATA IS NULL
   OR INTERACTIONS_WITH_CUSTOMER_SERVICE IS NULL
   OR PURCHASE_HISTORY IS NULL;
   
# Check for Integrity using md5 hash (Example using customer_behavior table)
SELECT 
    CUSTOMER_ID, 
    md5_number_lower64(
        TO_CHAR(CUSTOMER_ID) || 
        NVL(BEHAVIORAL_DATA, '') || 
        NVL(INTERACTIONS_WITH_CUSTOMER_SERVICE, '') || 
        TO_CHAR(PURCHASE_HISTORY, 'YYYY-MM-DD')
    ) AS calculated_md5_number,
    md5_hash 
FROM 
    CHALLENGE_DATABASE.CUSTOMER_SEGMENTATION_DEV.CUSTOMER_BEHAVIOR
WHERE 
    md5_number_lower64(
        TO_CHAR(CUSTOMER_ID) || 
        NVL(BEHAVIORAL_DATA, '') || 
        NVL(INTERACTIONS_WITH_CUSTOMER_SERVICE, '') || 
        TO_CHAR(PURCHASE_HISTORY, 'YYYY-MM-DD')
    ) != md5_hash;
```
**Note:** To automate this validation process, these SQL queries could be encapsulated within a Snowflake stored procedure. Additionally, a Snowflake task could be configured to automatically execute this procedure whenever there is a change in the relevant Snowflake tables. This setup would ensure continuous and automatic validation of data integrity, providing ongoing assurance that the data remains accurate over time.
## Table Normalization
When I decided to normalize the single, denormalized table into five distinct tables (**CUSTOMER_BEHAVIOR**, **CUSTOMER_DEMOGRAPHICS**, **CUSTOMER_INSURANCE_PRODUCTS**, **CUSTOMER_PREFERENCES**, and **CUSTOMER_SEGMENTATION**), my goal was to improve the database structure for better data management, performance, and maintainability. Here’s a breakdown of why I approached the normalization this way:
#### 1. **Logical Grouping of Related Data**
Each of the new tables represents a logical grouping of related data.
- **CUSTOMER_DEMOGRAPHICS:** Contains static information about the customer, such as age, gender, and geographic details.
- **CUSTOMER_BEHAVIOR:** Focuses on customer interactions and behavioral data, such as purchase history and interactions with customer service.
- **CUSTOMER_INSURANCE_PRODUCTS:** Captures details about the insurance products that the customer owns, including coverage amounts and premiums.
- **CUSTOMER_PREFERENCES:** Stores customer preferences, such as preferred communication channels and contact times.
- **CUSTOMER_SEGMENTATION:** Defines the segmentation of customers into different groups based on their characteristics or behaviors.

By grouping data logically, I ensured that each table contains data with a clear and focused purpose, which improves clarity and reduces the complexity of queries.

#### 2. **Scalability and Performance**
The normalized structure allows for more efficient data retrieval and updates:
- Queries can now target specific tables based on the required information, reducing the amount of unnecessary data processed.
- The separation of data into distinct tables allows for better indexing and query optimization, improving performance, especially as the dataset grows.

## Destroy Instructions
To clean up and remove all resources that were created as part of this project, you will need to sequentially destroy the infrastructure set up by both Terraform configurations. This ensures that all components, including the foundational setup and the main infrastructure, are properly and completely removed.
### Step 1: Deploy **terraform_setup** Infrastructure
```sh
cd terraform_setup
terraform destroy -var-file="variables.tfvars"
```
### Step 2: Deploy **0_terraform_setup** Infrastructure
```sh
cd 0_terraform_setup
terraform destroy -var-file="variables.tfvars"
```
By following these steps in order, you will ensure that all resources created by this project are fully decommissioned, leaving no residual components in your AWS or Snowflake environments.
